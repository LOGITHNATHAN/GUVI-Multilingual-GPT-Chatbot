# -*- coding: utf-8 -*-
"""Copy of Welcome To Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14OyYpvlJemPih6lRHoSckhhA1jC0pRHx
"""

pip install transformers datasets

import pandas as pd
import json

# Load Excel file
df = pd.read_excel("/content/guvi_qa_table.xlsx")

# Clean and convert to chat-style JSONL format
with open("guvi_chat_gpt2.jsonl", "w", encoding="utf-8") as f:
    for _, row in df.iterrows():
        question = str(row['Question']).strip()
        answer = str(row['Answer']).strip()
        chat_text = f"<|user|> {question} <|assistant|> {answer}"
        json.dump({"text": chat_text}, f)
        f.write("\n")

import os
os.environ["WANDB_DISABLED"] = "true"

from datasets import load_dataset
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments
import torch

# Load dataset
dataset = load_dataset("json", data_files="guvi_chat_gpt2.jsonl", split="train")

# Load tokenizer and model
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token  # Needed for padding
model = GPT2LMHeadModel.from_pretrained(model_name)

# Resize embedding if tokenizer updated
model.resize_token_embeddings(len(tokenizer))

# Tokenization
def tokenize(example):
    inputs = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=512
    )
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs

tokenized_dataset = dataset.map(tokenize, batched=True)
tokenized_dataset = tokenized_dataset.remove_columns(["text"])
tokenized_dataset.set_format("torch")

# Training arguments
training_args = TrainingArguments(
    output_dir="./gpt2-guvi-finetuned",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    fp16=True,
    report_to="none"
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

# Train
trainer.train()

# Save model and tokenizer after training
trainer.save_model("./gpt2-guvi-finetuned")             # saves model weights
tokenizer.save_pretrained("./gpt2-guvi-finetuned")      # saves tokenizer config

!zip -r gpt2-guvi-finetuned.zip gpt2-guvi-finetuned

from google.colab import drive
drive.mount('/content/drive')

!cp /content/gpt2-guvi-finetuned.zip /content/drive/MyDrive/

"""# Model predict"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("./gpt2-guvi-finetuned")

prompt = "<|user|> what courses offered? <|assistant|>"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("./gpt2-guvi-finetuned")

prompt = "<|user|>  placement support? <|assistant|>"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("./gpt2-guvi-finetuned")

prompt = "<|user|>   are guvi mentor supportive? <|assistant|>"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("./gpt2-guvi-finetuned")

prompt = "<|user|>   guvi certificate? <|assistant|>"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("./gpt2-guvi-finetuned")

prompt = "<|user|>   guvi mentors? <|assistant|>"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))

"""# Translator"""

!pip install transformers sentencepiece sacremoses torch

!pip install streamlit pyngrok transformers langdetect

import zipfile
import os

# Path to the ZIP file inside Google Drive
zip_path = '/content/gpt2-guvi-finetuned.zip'

# Destination path outside the drive (in local Colab filesystem)
extract_to = '/content/gpt2-guvi-finetuned'

# Create the directory if it doesn't exist
os.makedirs(extract_to, exist_ok=True)

# Extract
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_to)

print("Unzipped to:", extract_to)

import os
from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline
from langdetect import detect
import gradio as gr
import torch

# Set Hugging Face token for accessing gated models
os.environ["HF_TOKEN"] = "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"

# Load fine-tuned GPT-2 model and tokenizer
model_path = "/content/gpt2-guvi-finetuned/gpt2-guvi-finetuned"
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained(model_path)
model.eval()

# Translator to English (multi-language ‚Üí English)
translator_to_en = pipeline(
    "translation",
    model="Helsinki-NLP/opus-mt-mul-en",
    token=os.getenv("HF_TOKEN")
)

# Mapping English ‚Üí Indian language translators
def get_translator_from_en(lang_code):
    model_map = {
        "hi": "Helsinki-NLP/opus-mt-en-hi",     # Hindi
        "ta": "Helsinki-NLP/opus-mt-en-ta",    # Tamil
        "te": "Helsinki-NLP/opus-mt-en-tel",    # Telugu
        "kn": "Helsinki-NLP/opus-mt-en-kan",    # Kannada
        "ml": "Helsinki-NLP/opus-mt-en-mal",    # Malayalam
        "bn": "Helsinki-NLP/opus-mt-en-ben",    # Bengali
        "mr": "Helsinki-NLP/opus-mt-en-mar",    # Marathi
        "gu": "Helsinki-NLP/opus-mt-en-guj",    # Gujarati
        "pa": "Helsinki-NLP/opus-mt-en-pan",    # Punjabi
        "ur": "Helsinki-NLP/opus-mt-en-ur",     # Urdu
        "or": "Helsinki-NLP/opus-mt-en-ory",     # Odia
        "de": "Helsinki-NLP/opus-mt-en-de",     # German
        "ru": "Helsinki-NLP/opus-mt-en-ru"      # Russian
    }

    model_name = model_map.get(lang_code)
    return pipeline("translation", model=model_name, token=os.getenv("HF_TOKEN")) if model_name else None

# Generate GPT-2 response
def generate_response(prompt):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    output_ids = model.generate(
        input_ids,
        max_new_tokens=100,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(prompt, "").strip()

# Multilingual chat logic
def multilingual_chat(user_input):
    original_lang = detect(user_input)
    user_input_en = translator_to_en(user_input)[0]["translation_text"] if original_lang != "en" else user_input
    prompt = f"<|user|> {user_input_en} <|assistant|>"
    response_en = generate_response(prompt)

    if original_lang != "en":
        translator_back = get_translator_from_en(original_lang)
        if translator_back:
            return translator_back(response_en)[0]["translation_text"]
    return response_en

test_inputs = [
    "‡§Æ‡•Å‡§ù‡•á ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§™‡§§‡•ç‡§∞ ‡§ï‡•à‡§∏‡•á ‡§Æ‡§ø‡§≤‡•á‡§ó‡§æ?",             # Hindi
    "Tell me about GUVI Zen classes."            # English
]

for user_input in test_inputs:
    print("üó£ Input:", user_input)
    print("üîç Detected language:", detect(user_input))
    print("ü§ñ Chatbot response:", multilingual_chat(user_input))
    print("-" * 80)

test_inputs = [
    "GUVI Zen  ⁄©ŸÑÿßÿ≥ÿ≤ ŸÖ€å⁄∫ ⁄©€åÿß Ÿæ⁄ë⁄æÿß€åÿß ÿ¨ÿßÿ™ÿß €Å€íÿü"  # urudu
]

for user_input in test_inputs:
    print("üó£ Input:", user_input)
    print("üîç Detected language:", detect(user_input))
    print("ü§ñ Chatbot response:", multilingual_chat(user_input))
    print("-" * 80)

test_inputs = [
    "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç?",             # Russian
    "K√∂nnen Sie mir etwas √ºber die GUVI Zen-Kurse erz√§hlen?"            # German
]

for user_input in test_inputs:
    print("üó£ Input:", user_input)
    print("üîç Detected language:", detect(user_input))
    print("ü§ñ Chatbot response:", multilingual_chat(user_input))
    print("-" * 80)

"""# Streamlit"""

!pip install streamlit

!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os
# import torch
# import streamlit as st
# from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline
# from langdetect import detect
# 
# # --- Hugging Face Token ---
# os.environ["HF_TOKEN"] = "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"
# 
# # --- Streamlit Page Config ---
# st.set_page_config(page_title="GUVI Multilingual Chatbot", layout="wide")
# 
# # --- Sidebar ---
# st.sidebar.markdown("üß≠ Sidebar Navigation")
# page = st.sidebar.radio("Go to", ["Home", "Chatbot", "Report Page"])
# 
# # --- Load GPT-2 Fine-Tuned Model ---
# @st.cache_resource
# def load_model():
#     model_path = "/content/gpt2-guvi-finetuned/gpt2-guvi-finetuned"
#     tokenizer = GPT2Tokenizer.from_pretrained(model_path)
#     model = GPT2LMHeadModel.from_pretrained(model_path)
#     model.eval()
#     return tokenizer, model
# 
# tokenizer, model = load_model()
# 
# # --- Translation Pipeline: any ‚Üí English ---
# @st.cache_resource
# def load_translator_to_en():
#     return pipeline(
#         "translation",
#         model="Helsinki-NLP/opus-mt-mul-en",
#         token=os.getenv("HF_TOKEN")
#     )
# 
# translator_to_en = load_translator_to_en()
# 
# # --- English ‚Üí Other languages ---
# def get_translator_from_en(lang_code):
#     model_map = {
#         "hi": "Helsinki-NLP/opus-mt-en-hi",
#         "ta": "Helsinki-NLP/opus-mt-en-tam",
#         "te": "Helsinki-NLP/opus-mt-en-tel",
#         "kn": "Helsinki-NLP/opus-mt-en-kan",
#         "ml": "Helsinki-NLP/opus-mt-en-mal",
#         "bn": "Helsinki-NLP/opus-mt-en-ben",
#         "mr": "Helsinki-NLP/opus-mt-en-mar",
#         "gu": "Helsinki-NLP/opus-mt-en-guj",
#         "pa": "Helsinki-NLP/opus-mt-en-pan",
#         "ur": "Helsinki-NLP/opus-mt-en-ur",
#         "or": "Helsinki-NLP/opus-mt-en-ory",
#         "de": "Helsinki-NLP/opus-mt-en-de",
#         "ru": "Helsinki-NLP/opus-mt-en-ru"
#     }
#     model_name = model_map.get(lang_code)
#     return pipeline("translation", model=model_name, token=os.getenv("HF_TOKEN")) if model_name else None
# 
# # --- Generate GPT Response ---
# def generate_response(prompt):
#     input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
#     output_ids = model.generate(input_ids, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)
#     return tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(prompt, "").strip()
# 
# # --- Chat Pipeline ---
# def multilingual_chat(user_input):
#     original_lang = detect(user_input)
#     user_input_en = translator_to_en(user_input)[0]["translation_text"] if original_lang != "en" else user_input
#     prompt = f"<|user|> {user_input_en} <|assistant|>"
#     response_en = generate_response(prompt)
# 
#     if original_lang != "en":
#         translator_back = get_translator_from_en(original_lang)
#         if translator_back:
#             return translator_back(response_en)[0]["translation_text"]
#     return response_en
# 
# # --- Home Page ---
# if page == "Home":
#     st.header("üè† GUVI Multilingual GPT Chatbot using Streamlit ‚Äì Integrated Translation and Domain-Specific Model Deployment")
#     st.markdown("""
#     This is an integrated chatbot built using:
#     - GPT-2 Fine-Tuned Model
#     - Helsinki-NLP Translation Pipelines
#     - Deployed in Hugging Face
# 
#     **Navigation:**
#     Use the sidebar to access the Chatbot or view the Report Page.
#     """)
# 
#     st.markdown("### üîó Links")
#     st.markdown("""
#     - [üíª GitHub Repository](https://github.com/LOGITHNATHAN/GUVI-Multilingual-GPT-Chatbot)
#     - [üöÄ Live Demo on Hugging Face Spaces](https://huggingface.co/spaces/logith02/guvi_chatbot4)
#     """)
# 
# # --- Chatbot Page ---
# elif page == "Chatbot":
#     st.title("üó£Ô∏è Multilingual Chatbot for GUVI Learners")
#     st.markdown("Ask questions in English, Hindi, Urdu, Russian, or German.")
# 
#     if "history" not in st.session_state:
#         st.session_state.history = []
# 
#     # --- Display Chat History ---
#     for i, (user, bot) in enumerate(st.session_state.history):
#         with st.chat_message("user", avatar="üë§"):
#             st.markdown(user)
#         with st.chat_message("assistant", avatar="ü§ñ"):
#             st.markdown(bot)
# 
#     # --- Chat Input ---
#     user_input = st.chat_input("Type your message here...")
#     if user_input:
#         response = multilingual_chat(user_input)
#         st.session_state.history.append((user_input, response))
#         st.rerun()
# 
#     # --- Divider ---
#     st.markdown("---")
#     st.markdown("### ü§ñ Ask a Frequently Asked Question")
# 
#     # --- FAQ Lists ---
#     faqs_english = [
#         "What is GUVI?",
#         "What is the main goal of GUVI?",
#         "Guvi placement details?",
#         "What courses are offered in GUVI?",
#         "Guvi recruiters?"
#     ]
# 
#     faqs_hindi = [
#         "GUVI ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?",
#         "GUVI ‡§ï‡§æ ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§â‡§¶‡•ç‡§¶‡•á‡§∂‡•ç‡§Ø ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?",
#         "GUVI ‡§ï‡§ø‡§® ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ï‡•ã‡§∞‡•ç‡§∏ ‡§™‡•ç‡§∞‡§¶‡§æ‡§® ‡§ï‡§∞‡§§‡§æ ‡§π‡•à?",
#         "GUVI ‡§Æ‡•á‡§Ç ‡§ï‡§ø‡§∏ ‡§™‡•ç‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•Ä ‡§∂‡§ø‡§ï‡•ç‡§∑‡§£ ‡§µ‡§ø‡§ß‡§ø‡§Ø‡§æ‡§Å ‡§π‡•à‡§Ç?",
#         "GUVI ‡§ï‡•á ‡§Æ‡•Å‡§´‡§º‡•ç‡§§ ‡§ï‡•ã‡§∞‡•ç‡§∏ ‡§ï‡•å‡§® ‡§∏‡•á ‡§π‡•à‡§Ç?"
#     ]
# 
#     faqs_urdu = [
#         "GUVI ⁄©€åÿß €Å€íÿü",
#         "GUVI ⁄©ÿß ÿ®ŸÜ€åÿßÿØ€å ŸÖŸÇÿµÿØ ⁄©€åÿß €Å€íÿü",
#         "GUVI ⁄©€å ŸÖŸÑÿßÿ≤ŸÖÿ™ ⁄©€å ÿ™ŸÅÿµ€åŸÑÿßÿ™ÿü",
#         "GUVI ŸÖ€å⁄∫ ÿ≥€å⁄©⁄æŸÜ€í ⁄©€í ⁄©ŸàŸÜ ÿ≥€í ÿ∑ÿ±€åŸÇ€í €Å€å⁄∫ÿü",
#         "GUVI ⁄©€í ⁄©ŸàŸÜ ÿ≥€í ÿ±€å⁄©ÿ±ŸàŸπÿ±ÿ≤ €Å€å⁄∫ÿü"
#     ]
# 
#     faqs_russian = [
#         "–ß—Ç–æ —Ç–∞–∫–æ–µ GUVI?",
#         "–ö–∞–∫–æ–≤–∞ –æ—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å GUVI?",
#         "–î–µ—Ç–∞–ª–∏ —Ç—Ä—É–¥–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –æ—Ç GUVI?",
#         "–ö–∞–∫–∏–µ —Ñ–æ—Ä–º—ã –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç GUVI?",
#         "–ö–∞–∫–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞–Ω–∏–º–∞—é—Ç —á–µ—Ä–µ–∑ GUVI?"
#     ]
# 
#     faqs_german = [
#         "Was ist GUVI?",
#         "Was ist das Hauptziel von GUVI?",
#         "Informationen zur Vermittlung bei GUVI?",
#         "Welche Lernmethoden bietet GUVI an?",
#         "Welche Unternehmen rekrutieren √ºber GUVI?"
#     ]
# 
#     # --- FAQ Dropdowns ---
#     selected_en = st.selectbox("üìò Select an English FAQ", ["-- Select --"] + faqs_english, key="faq_en")
#     if selected_en and selected_en != "-- Select --":
#         response = multilingual_chat(selected_en)
#         st.success(f"**Answer:** {response}")
# 
#     selected_hi = st.selectbox("üìô ‡§ï‡•ã‡§à ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§ö‡•Å‡§®‡•á‡§Ç", ["-- ‡§ö‡•Å‡§®‡•á‡§Ç --"] + faqs_hindi, key="faq_hi")
#     if selected_hi and selected_hi != "-- ‡§ö‡•Å‡§®‡•á‡§Ç --":
#         response = multilingual_chat(selected_hi)
#         st.success(f"**‡§â‡§§‡•ç‡§§‡§∞:** {response}")
# 
#     selected_ur = st.selectbox("üìó ÿß€å⁄© ÿ≥ŸàÿßŸÑ ŸÖŸÜÿ™ÿÆÿ® ⁄©ÿ±€å⁄∫ (ÿßÿ±ÿØŸà)", ["-- ŸÖŸÜÿ™ÿÆÿ® ⁄©ÿ±€å⁄∫ --"] + faqs_urdu, key="faq_ur")
#     if selected_ur and selected_ur != "-- ŸÖŸÜÿ™ÿÆÿ® ⁄©ÿ±€å⁄∫ --":
#         response = multilingual_chat(selected_ur)
#         st.success(f"**ÿ¨Ÿàÿßÿ®:** {response}")
# 
#     selected_ru = st.selectbox("üìí –í—ã–±–µ—Ä–∏—Ç–µ –≤–æ–ø—Ä–æ—Å (Russian)", ["-- –í—ã–±—Ä–∞—Ç—å --"] + faqs_russian, key="faq_ru")
#     if selected_ru and selected_ru != "-- –í—ã–±—Ä–∞—Ç—å --":
#         response = multilingual_chat(selected_ru)
#         st.success(f"**–û—Ç–≤–µ—Ç:** {response}")
# 
#     selected_de = st.selectbox("üìï W√§hlen Sie eine Frage (German)", ["-- W√§hlen --"] + faqs_german, key="faq_de")
#     if selected_de and selected_de != "-- W√§hlen --":
#         response = multilingual_chat(selected_de)
#         st.success(f"**Antwort:** {response}")
# 
# # --- Report Page ---
# elif page == "Report Page":
#     st.title("üìä Report Page")
#         # New Section - GUVI Multilingual Chatbot Project Report
#     st.markdown("## üßæ GUVI Multilingual GPT Chatbot Report")
# 
#     st.markdown("""
# ### üß† Objective
# To build a multilingual AI-powered chatbot using a fine-tuned GPT-2 model that supports real-time interactions in multiple languages, helping GUVI learners access information in their native language.
# 
# ### üåê Domain
# - Artificial Intelligence (AI)
# - Natural Language Processing (NLP)
# - Web Development (Streamlit)
# 
# ### ‚ùì Problem Statement
# Build a chatbot that:
# - Accepts user input in Indian/international languages
# - Translates to English
# - Generates GUVI-specific responses
# - Translates back to the original language
# - Presents results via a Streamlit interface
# 
# ### üíº Business Use Cases
# 1. Customer Support Automation
# 2. E-Learning Accessibility
# 3. Career Guidance & Mentorship
# 4. Course Recommendation System
# 
# ### ‚ú® Key Features
# - Language detection (`langdetect`)
# - Translation (‚Üî English) via Hugging Face models
# - GPT-2 based response generation
# - Multilingual FAQ support
# - Persistent chat history using `st.session_state`
# 
# ### üõ†Ô∏è Tools & Technologies
# - Python 3.11
# - Hugging Face Transformers
# - Streamlit
# - Langdetect
# - GitHub, VS Code / Jupyter
# 
# ### üèóÔ∏è Architecture
# 1. User Input Layer
# 2. Language Detection
# 3. Translation Layer
# 4. GPT-2 Response Generation
# 5. Streamlit UI Rendering
# 
# ### üîÅ Fine-Tuning Process
# 1. Data Collection
# 2. Preprocessing
# 3. Tokenization
# 4. Fine-Tuning GPT-2
# 5. Evaluation
# 
# ### ‚úÖ Results
# - Multilingual chatbot with real-time support in 10+ languages
# - Dynamic FAQs auto-translated
# - Domain-specific contextual answers
# - Extensible design for future languages or enterprises
# 
# ### üìä Evaluation Metrics
# - Code Modularity
# - Multilingual Capability
# - Response Clarity
# - UI Responsiveness
# - GitHub Documentation Quality
# 
# ### üßæ Dataset Info
# - GUVI-specific textual content
# - Hugging Face translation datasets
# - ~13,500 tokens
# 
# ### üì¶ Deliverables
# - `app.py`
# - `requirements.txt`
# - `README.md`
# - GitHub Repository
# - (Optional) Demo Video
# 
# ### üìà Suggested Improvements
# - Add speech-to-text / text-to-speech
# - Add analytics dashboard
# - Add user feedback system
# - Optimize for mobile
# - Support additional languages
# 
# ### üöÄ Deployment
# - Hugging Face Spaces
# - Secure token handling
# - Cached model loading for faster inference
# 
# ### üèÅ Conclusion
# This project demonstrates how deep learning, NLP, and translation can be integrated into a multilingual chatbot tailored for GUVI. It improves learner accessibility, engagement, and personalization.
# """)
# 
#

!npm install localtunnel

!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com

"""# Translator IndicTrans2"""

pip install transformers torch gradio langdetect

!huggingface-cli login

import os
os.environ.pop("HF_TOKEN", None)  # remove stale env var so it stops overriding login

from huggingface_hub import login, whoami

login(token="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", add_to_git_credential=True)  # no env var needed
print(whoami())  # should print your username/info, no 401

# Translation pipelines using IndicTrans2
def translate_to_en(text, src_lang):
    # Append target language token <2en>
    text_with_lang = text + " <2en>"
    inputs = indic_to_en_tokenizer(text_with_lang, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        output = indic_to_en_model.generate(**inputs, max_length=256)
    return indic_to_en_tokenizer.decode(output[0], skip_special_tokens=True)

def translate_from_en(text, tgt_lang):
    # Append target language token like <2hi>, <2ta>, etc.
    text_with_lang = text + f" <2{tgt_lang}>"
    inputs = en_to_indic_tokenizer(text_with_lang, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        output = en_to_indic_model.generate(**inputs, max_length=256)
    return en_to_indic_tokenizer.decode(output[0], skip_special_tokens=True)

"""# Translator NLLB"""

!pip install langdetect

pip install transformers sentencepiece

pip install --upgrade transformers

import transformers
print(transformers.__version__)

from transformers import AutoTokenizer

model_id = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_id)

# The mapping is stored in tokenizer.additional_special_tokens and tokenizer.all_special_tokens
print("Tokenizer Type:", type(tokenizer))
print("Available language codes:", tokenizer.additional_special_tokens[:10])  # show first 10 codes

# Access lang_code_to_id from tokenizer
if hasattr(tokenizer, "lang_code_to_id"):
    print("Tamil lang ID:", tokenizer.lang_code_to_id["tam_Taml"])
else:
    print("Tokenizer does not have lang_code_to_id, using config instead...")
    print("Example Tamil:", tokenizer.convert_tokens_to_ids("tam_Taml"))

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from transformers import AutoModelForSeq2SeqLM, NllbTokenizer
from langdetect import detect

# ========== Load GPT-2 Fine-tuned Model ==========
gpt2_model_path = "./gpt2-guvi-finetuned"  # Update to your path
gpt2_tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
gpt2_model = GPT2LMHeadModel.from_pretrained(gpt2_model_path)
gpt2_model.eval()

# ========== Load NLLB-200 Translation Model ==========
nllb_model_id = "facebook/nllb-200-distilled-600M"
nllb_tokenizer = NllbTokenizer.from_pretrained(nllb_model_id)
nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_id)

# ========== Language Map (ISO code ‚ûù NLLB code) ==========
lang_map = {
    "as": "asm_Beng",   # Assamese
    "bn": "ben_Beng",   # Bengali
    "gu": "guj_Gujr",   # Gujarati
    "hi": "hin_Deva",   # Hindi
    "kn": "kan_Knda",   # Kannada
    "ml": "mal_Mlym",   # Malayalam
    "mr": "mar_Deva",   # Marathi
    "ne": "npi_Deva",   # Nepali
    "or": "ory_Orya",   # Odia
    "pa": "pan_Guru",   # Punjabi
    "sa": "san_Deva",   # Sanskrit
    "ta": "tam_Taml",   # Tamil
    "te": "tel_Telu",   # Telugu
    "ur": "urd_Arab",   # Urdu
    "ks": "kas_Arab",   # Kashmiri
    "sd": "snd_Arab",   # Sindhi
    "kok": "kok_Deva",  # Konkani
    "mai": "mai_Deva",  # Maithili
    "bho": "bho_Deva",  # Bhojpuri
    "mni": "mni_Beng",  # Manipuri
    "doi": "doi_Deva",  # Dogri
    "en": "eng_Latn"    # English
}

# ========== Translate using NLLB ==========
def translate_nllb(text, src_lang_code, tgt_lang_code):
    inputs = nllb_tokenizer(text, return_tensors="pt")
    inputs["forced_bos_token_id"] = nllb_tokenizer.lang_code_to_id[tgt_lang_code]
    with torch.no_grad():
        output_tokens = nllb_model.generate(**inputs, max_new_tokens=200)
    return nllb_tokenizer.decode(output_tokens[0], skip_special_tokens=True)

# ========== Generate GPT-2 Response ==========
def generate_response(prompt):
    input_ids = gpt2_tokenizer(prompt, return_tensors="pt").input_ids.to(gpt2_model.device)
    with torch.no_grad():
        output_ids = gpt2_model.generate(
            input_ids,
            max_new_tokens=100,
            pad_token_id=gpt2_tokenizer.eos_token_id
        )
    return gpt2_tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(prompt, "").strip()

# ========== Chatbot Logic ==========
def multilingual_chat(user_input):
    # Step 1: Detect language
    detected_lang = detect(user_input)
    src_lang = lang_map.get(detected_lang, "eng_Latn")  # fallback to English

    # Step 2: Translate to English if needed
    if detected_lang != "en":
        user_input_en = translate_nllb(user_input, src_lang, "eng_Latn")
    else:
        user_input_en = user_input

    # Step 3: Format prompt and get GPT-2 response
    prompt = f"<|user|> {user_input_en} <|assistant|>"
    response_en = generate_response(prompt)

    # Step 4: Translate back to original language if needed
    if detected_lang != "en":
        response_final = translate_nllb(response_en, "eng_Latn", src_lang)
    else:
        response_final = response_en

    return response_final

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "facebook/nllb-200-distilled-600M"

# Initialize tokenizer with source language
tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang="tam_Taml")
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def multilingual_chat(text, target_lang="eng_Latn"):
    inputs = tokenizer(text, return_tensors="pt")
    translated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id[target_lang]  # target language
    )
    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

from transformers import AutoTokenizer

model_id = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)  # <- disable fast tokenizer

print("Tokenizer Type:", type(tokenizer))
print("Has lang_code_to_id:", hasattr(tokenizer, "lang_code_to_id"))
print("Example (Tamil):", tokenizer.lang_code_to_id["tam_Taml"])

import os
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langdetect import detect

# Load fine-tuned GPT-2 model and tokenizer
model_path = "./gpt2-guvi-finetuned"
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained(model_path)
model.eval()

# Load NLLB model and tokenizer
nllb_model_id = "facebook/nllb-200-distilled-600M"
nllb_tokenizer = AutoTokenizer.from_pretrained(nllb_model_id)
nllb_model = AutoModelForSeq2SeqLM.from_pretrained(nllb_model_id)

# Indian language codes for NLLB
lang_codes = {
    "hi": "hin_Deva",   # Hindi
    "ta": "tam_Taml",   # Tamil
    "te": "tel_Telu",   # Telugu
    "kn": "kan_Knda",   # Kannada
    "ml": "mal_Mlym",   # Malayalam
    "bn": "ben_Beng",   # Bengali
    "mr": "mar_Deva",   # Marathi
    "gu": "guj_Gujr",   # Gujarati
    "pa": "pan_Guru",   # Punjabi
    "ur": "urd_Arab",   # Urdu
    "or": "ory_Orya",   # Odia
    "en": "eng_Latn"    # English
}

# Use NLLB for translation between any two languages
def translate_with_nllb(text, source_lang, target_lang):
    inputs = nllb_tokenizer(text, return_tensors="pt", padding=True, truncation=True).to(nllb_model.device)
    inputs['forced_bos_token_id'] = nllb_tokenizer.lang_code_to_id[target_lang]
    outputs = nllb_model.generate(**inputs, max_new_tokens=256)
    return nllb_tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]

# Generate GPT-2 response
def generate_response(prompt):
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(model.device)
    output_ids = model.generate(
        input_ids,
        max_new_tokens=100,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(output_ids[0], skip_special_tokens=True).replace(prompt, "").strip()

# Multilingual chat logic
def multilingual_chat(user_input):
    original_lang = detect(user_input)
    print(f"üîç Detected: {original_lang}")

    if original_lang not in lang_codes:
        return "‚ùó Unsupported language"

    # Translate to English if needed
    if original_lang != "en":
        user_input_en = translate_with_nllb(user_input, lang_codes[original_lang], lang_codes["en"])
    else:
        user_input_en = user_input

    # Generate response in English
    prompt = f"<|user|> {user_input_en} <|assistant|>"
    response_en = generate_response(prompt)

    # Translate back to original language
    if original_lang != "en":
        response_translated = translate_with_nllb(response_en, lang_codes["en"], lang_codes[original_lang])
    else:
        response_translated = response_en

    return response_translated

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load correct NLLB model
model_name = "facebook/nllb-200-distilled-600M"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)  # disable fast tokenizer
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def multilingual_chat(text, src_lang="tam_Taml", tgt_lang="eng_Latn"):
    inputs = tokenizer(text, return_tensors="pt")
    translated_tokens = model.generate(
        **inputs,
        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang]  # needs lang_code_to_id
    )
    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

# Tamil
print(multilingual_chat("‡Æö‡Æ®‡Øç‡Æ§‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æ™‡Øà ‡Æé‡Æ©‡Øç‡Æ© ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç?", src_lang="tam_Taml", tgt_lang="eng_Latn"))

# Hindi
print(multilingual_chat("‡§ó‡•Å‡§µ‡•Ä ‡§ï‡•ç‡§≤‡§æ‡§∏ ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¨‡§§‡§æ‡§ì", src_lang="hin_Deva", tgt_lang="eng_Latn"))

# English (just passes through)
print(multilingual_chat("Tell me about GUVI Zen class", src_lang="eng_Latn", tgt_lang="eng_Latn"))

